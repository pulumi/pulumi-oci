// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Oci.AiLanguage.Inputs
{

    public sealed class ModelEvaluationResultClassMetricArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// F1-score, is a measure of a modelâ€™s accuracy on a dataset
        /// </summary>
        [Input("f1")]
        public Input<double>? F1 { get; set; }

        /// <summary>
        /// Entity label
        /// </summary>
        [Input("label")]
        public Input<string>? Label { get; set; }

        /// <summary>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </summary>
        [Input("precision")]
        public Input<double>? Precision { get; set; }

        /// <summary>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </summary>
        [Input("recall")]
        public Input<double>? Recall { get; set; }

        /// <summary>
        /// number of samples in the test set
        /// </summary>
        [Input("support")]
        public Input<double>? Support { get; set; }

        public ModelEvaluationResultClassMetricArgs()
        {
        }
        public static new ModelEvaluationResultClassMetricArgs Empty => new ModelEvaluationResultClassMetricArgs();
    }
}
