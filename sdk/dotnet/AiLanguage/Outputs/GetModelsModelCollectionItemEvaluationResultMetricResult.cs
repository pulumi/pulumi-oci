// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Oci.AiLanguage.Outputs
{

    [OutputType]
    public sealed class GetModelsModelCollectionItemEvaluationResultMetricResult
    {
        /// <summary>
        /// The fraction of the labels that were correctly recognised .
        /// </summary>
        public readonly double? Accuracy;
        /// <summary>
        /// F1-score, is a measure of a model’s accuracy on a dataset
        /// </summary>
        public readonly double? MacroF1;
        /// <summary>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </summary>
        public readonly double? MacroPrecision;
        /// <summary>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </summary>
        public readonly double? MacroRecall;
        /// <summary>
        /// F1-score, is a measure of a model’s accuracy on a dataset
        /// </summary>
        public readonly double? MicroF1;
        /// <summary>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </summary>
        public readonly double? MicroPrecision;
        /// <summary>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </summary>
        public readonly double? MicroRecall;
        /// <summary>
        /// F1-score, is a measure of a model’s accuracy on a dataset
        /// </summary>
        public readonly double? WeightedF1;
        /// <summary>
        /// Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
        /// </summary>
        public readonly double? WeightedPrecision;
        /// <summary>
        /// Measures the model's ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
        /// </summary>
        public readonly double? WeightedRecall;

        [OutputConstructor]
        private GetModelsModelCollectionItemEvaluationResultMetricResult(
            double? accuracy,

            double? macroF1,

            double? macroPrecision,

            double? macroRecall,

            double? microF1,

            double? microPrecision,

            double? microRecall,

            double? weightedF1,

            double? weightedPrecision,

            double? weightedRecall)
        {
            Accuracy = accuracy;
            MacroF1 = macroF1;
            MacroPrecision = macroPrecision;
            MacroRecall = macroRecall;
            MicroF1 = microF1;
            MicroPrecision = microPrecision;
            MicroRecall = microRecall;
            WeightedF1 = weightedF1;
            WeightedPrecision = weightedPrecision;
            WeightedRecall = weightedRecall;
        }
    }
}
