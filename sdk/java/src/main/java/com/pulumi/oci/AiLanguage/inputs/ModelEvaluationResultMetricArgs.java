// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.oci.AiLanguage.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import java.lang.Double;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class ModelEvaluationResultMetricArgs extends com.pulumi.resources.ResourceArgs {

    public static final ModelEvaluationResultMetricArgs Empty = new ModelEvaluationResultMetricArgs();

    /**
     * The fraction of the labels that were correctly recognised .
     * 
     */
    @Import(name="accuracy")
    private @Nullable Output<Double> accuracy;

    /**
     * @return The fraction of the labels that were correctly recognised .
     * 
     */
    public Optional<Output<Double>> accuracy() {
        return Optional.ofNullable(this.accuracy);
    }

    /**
     * F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    @Import(name="macroF1")
    private @Nullable Output<Double> macroF1;

    /**
     * @return F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    public Optional<Output<Double>> macroF1() {
        return Optional.ofNullable(this.macroF1);
    }

    /**
     * Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    @Import(name="macroPrecision")
    private @Nullable Output<Double> macroPrecision;

    /**
     * @return Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    public Optional<Output<Double>> macroPrecision() {
        return Optional.ofNullable(this.macroPrecision);
    }

    /**
     * Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    @Import(name="macroRecall")
    private @Nullable Output<Double> macroRecall;

    /**
     * @return Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    public Optional<Output<Double>> macroRecall() {
        return Optional.ofNullable(this.macroRecall);
    }

    /**
     * F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    @Import(name="microF1")
    private @Nullable Output<Double> microF1;

    /**
     * @return F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    public Optional<Output<Double>> microF1() {
        return Optional.ofNullable(this.microF1);
    }

    /**
     * Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    @Import(name="microPrecision")
    private @Nullable Output<Double> microPrecision;

    /**
     * @return Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    public Optional<Output<Double>> microPrecision() {
        return Optional.ofNullable(this.microPrecision);
    }

    /**
     * Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    @Import(name="microRecall")
    private @Nullable Output<Double> microRecall;

    /**
     * @return Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    public Optional<Output<Double>> microRecall() {
        return Optional.ofNullable(this.microRecall);
    }

    /**
     * F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    @Import(name="weightedF1")
    private @Nullable Output<Double> weightedF1;

    /**
     * @return F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    public Optional<Output<Double>> weightedF1() {
        return Optional.ofNullable(this.weightedF1);
    }

    /**
     * Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    @Import(name="weightedPrecision")
    private @Nullable Output<Double> weightedPrecision;

    /**
     * @return Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    public Optional<Output<Double>> weightedPrecision() {
        return Optional.ofNullable(this.weightedPrecision);
    }

    /**
     * Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    @Import(name="weightedRecall")
    private @Nullable Output<Double> weightedRecall;

    /**
     * @return Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    public Optional<Output<Double>> weightedRecall() {
        return Optional.ofNullable(this.weightedRecall);
    }

    private ModelEvaluationResultMetricArgs() {}

    private ModelEvaluationResultMetricArgs(ModelEvaluationResultMetricArgs $) {
        this.accuracy = $.accuracy;
        this.macroF1 = $.macroF1;
        this.macroPrecision = $.macroPrecision;
        this.macroRecall = $.macroRecall;
        this.microF1 = $.microF1;
        this.microPrecision = $.microPrecision;
        this.microRecall = $.microRecall;
        this.weightedF1 = $.weightedF1;
        this.weightedPrecision = $.weightedPrecision;
        this.weightedRecall = $.weightedRecall;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(ModelEvaluationResultMetricArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private ModelEvaluationResultMetricArgs $;

        public Builder() {
            $ = new ModelEvaluationResultMetricArgs();
        }

        public Builder(ModelEvaluationResultMetricArgs defaults) {
            $ = new ModelEvaluationResultMetricArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param accuracy The fraction of the labels that were correctly recognised .
         * 
         * @return builder
         * 
         */
        public Builder accuracy(@Nullable Output<Double> accuracy) {
            $.accuracy = accuracy;
            return this;
        }

        /**
         * @param accuracy The fraction of the labels that were correctly recognised .
         * 
         * @return builder
         * 
         */
        public Builder accuracy(Double accuracy) {
            return accuracy(Output.of(accuracy));
        }

        /**
         * @param macroF1 F1-score, is a measure of a model’s accuracy on a dataset
         * 
         * @return builder
         * 
         */
        public Builder macroF1(@Nullable Output<Double> macroF1) {
            $.macroF1 = macroF1;
            return this;
        }

        /**
         * @param macroF1 F1-score, is a measure of a model’s accuracy on a dataset
         * 
         * @return builder
         * 
         */
        public Builder macroF1(Double macroF1) {
            return macroF1(Output.of(macroF1));
        }

        /**
         * @param macroPrecision Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
         * 
         * @return builder
         * 
         */
        public Builder macroPrecision(@Nullable Output<Double> macroPrecision) {
            $.macroPrecision = macroPrecision;
            return this;
        }

        /**
         * @param macroPrecision Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
         * 
         * @return builder
         * 
         */
        public Builder macroPrecision(Double macroPrecision) {
            return macroPrecision(Output.of(macroPrecision));
        }

        /**
         * @param macroRecall Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
         * 
         * @return builder
         * 
         */
        public Builder macroRecall(@Nullable Output<Double> macroRecall) {
            $.macroRecall = macroRecall;
            return this;
        }

        /**
         * @param macroRecall Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
         * 
         * @return builder
         * 
         */
        public Builder macroRecall(Double macroRecall) {
            return macroRecall(Output.of(macroRecall));
        }

        /**
         * @param microF1 F1-score, is a measure of a model’s accuracy on a dataset
         * 
         * @return builder
         * 
         */
        public Builder microF1(@Nullable Output<Double> microF1) {
            $.microF1 = microF1;
            return this;
        }

        /**
         * @param microF1 F1-score, is a measure of a model’s accuracy on a dataset
         * 
         * @return builder
         * 
         */
        public Builder microF1(Double microF1) {
            return microF1(Output.of(microF1));
        }

        /**
         * @param microPrecision Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
         * 
         * @return builder
         * 
         */
        public Builder microPrecision(@Nullable Output<Double> microPrecision) {
            $.microPrecision = microPrecision;
            return this;
        }

        /**
         * @param microPrecision Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
         * 
         * @return builder
         * 
         */
        public Builder microPrecision(Double microPrecision) {
            return microPrecision(Output.of(microPrecision));
        }

        /**
         * @param microRecall Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
         * 
         * @return builder
         * 
         */
        public Builder microRecall(@Nullable Output<Double> microRecall) {
            $.microRecall = microRecall;
            return this;
        }

        /**
         * @param microRecall Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
         * 
         * @return builder
         * 
         */
        public Builder microRecall(Double microRecall) {
            return microRecall(Output.of(microRecall));
        }

        /**
         * @param weightedF1 F1-score, is a measure of a model’s accuracy on a dataset
         * 
         * @return builder
         * 
         */
        public Builder weightedF1(@Nullable Output<Double> weightedF1) {
            $.weightedF1 = weightedF1;
            return this;
        }

        /**
         * @param weightedF1 F1-score, is a measure of a model’s accuracy on a dataset
         * 
         * @return builder
         * 
         */
        public Builder weightedF1(Double weightedF1) {
            return weightedF1(Output.of(weightedF1));
        }

        /**
         * @param weightedPrecision Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
         * 
         * @return builder
         * 
         */
        public Builder weightedPrecision(@Nullable Output<Double> weightedPrecision) {
            $.weightedPrecision = weightedPrecision;
            return this;
        }

        /**
         * @param weightedPrecision Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
         * 
         * @return builder
         * 
         */
        public Builder weightedPrecision(Double weightedPrecision) {
            return weightedPrecision(Output.of(weightedPrecision));
        }

        /**
         * @param weightedRecall Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
         * 
         * @return builder
         * 
         */
        public Builder weightedRecall(@Nullable Output<Double> weightedRecall) {
            $.weightedRecall = weightedRecall;
            return this;
        }

        /**
         * @param weightedRecall Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
         * 
         * @return builder
         * 
         */
        public Builder weightedRecall(Double weightedRecall) {
            return weightedRecall(Output.of(weightedRecall));
        }

        public ModelEvaluationResultMetricArgs build() {
            return $;
        }
    }

}
