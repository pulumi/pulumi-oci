// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.oci.AiLanguage.outputs;

import com.pulumi.core.annotations.CustomType;
import java.lang.Double;
import java.lang.String;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class GetModelsModelCollectionItemEvaluationResultEntityMetric {
    /**
     * @return F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    private @Nullable Double f1;
    /**
     * @return Entity label
     * 
     */
    private @Nullable String label;
    /**
     * @return Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    private @Nullable Double precision;
    /**
     * @return Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    private @Nullable Double recall;

    private GetModelsModelCollectionItemEvaluationResultEntityMetric() {}
    /**
     * @return F1-score, is a measure of a model’s accuracy on a dataset
     * 
     */
    public Optional<Double> f1() {
        return Optional.ofNullable(this.f1);
    }
    /**
     * @return Entity label
     * 
     */
    public Optional<String> label() {
        return Optional.ofNullable(this.label);
    }
    /**
     * @return Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives)
     * 
     */
    public Optional<Double> precision() {
        return Optional.ofNullable(this.precision);
    }
    /**
     * @return Measures the model&#39;s ability to predict actual positive classes. It is the ratio between the predicted true positives and what was actually tagged. The recall metric reveals how many of the predicted classes are correct.
     * 
     */
    public Optional<Double> recall() {
        return Optional.ofNullable(this.recall);
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(GetModelsModelCollectionItemEvaluationResultEntityMetric defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private @Nullable Double f1;
        private @Nullable String label;
        private @Nullable Double precision;
        private @Nullable Double recall;
        public Builder() {}
        public Builder(GetModelsModelCollectionItemEvaluationResultEntityMetric defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.f1 = defaults.f1;
    	      this.label = defaults.label;
    	      this.precision = defaults.precision;
    	      this.recall = defaults.recall;
        }

        @CustomType.Setter
        public Builder f1(@Nullable Double f1) {
            this.f1 = f1;
            return this;
        }
        @CustomType.Setter
        public Builder label(@Nullable String label) {
            this.label = label;
            return this;
        }
        @CustomType.Setter
        public Builder precision(@Nullable Double precision) {
            this.precision = precision;
            return this;
        }
        @CustomType.Setter
        public Builder recall(@Nullable Double recall) {
            this.recall = recall;
            return this;
        }
        public GetModelsModelCollectionItemEvaluationResultEntityMetric build() {
            final var o = new GetModelsModelCollectionItemEvaluationResultEntityMetric();
            o.f1 = f1;
            o.label = label;
            o.precision = precision;
            o.recall = recall;
            return o;
        }
    }
}
