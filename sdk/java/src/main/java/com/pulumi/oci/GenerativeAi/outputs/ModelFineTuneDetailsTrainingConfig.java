// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.oci.GenerativeAi.outputs;

import com.pulumi.core.annotations.CustomType;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Double;
import java.lang.Integer;
import java.lang.String;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class ModelFineTuneDetailsTrainingConfig {
    /**
     * @return Stop training if the loss metric does not improve beyond &#39;early_stopping_threshold&#39; for this many times of evaluation.
     * 
     */
    private @Nullable Integer earlyStoppingPatience;
    /**
     * @return How much the loss must improve to prevent early stopping.
     * 
     */
    private @Nullable Double earlyStoppingThreshold;
    /**
     * @return The initial learning rate to be used during training
     * 
     */
    private @Nullable Double learningRate;
    /**
     * @return Determines how frequently to log model metrics.
     * 
     * Every step is logged for the first 20 steps and then follows this parameter for log frequency. Set to 0 to disable logging the model metrics.
     * 
     */
    private @Nullable Integer logModelMetricsIntervalInSteps;
    /**
     * @return This parameter represents the scaling factor for the weight matrices in LoRA.
     * 
     */
    private @Nullable Integer loraAlpha;
    /**
     * @return This parameter indicates the dropout probability for LoRA layers.
     * 
     */
    private @Nullable Double loraDropout;
    /**
     * @return This parameter represents the LoRA rank of the update matrices.
     * 
     */
    private @Nullable Integer loraR;
    /**
     * @return The number of last layers to be fine-tuned.
     * 
     */
    private @Nullable Integer numOfLastLayers;
    /**
     * @return The maximum number of training epochs to run for.
     * 
     */
    private @Nullable Integer totalTrainingEpochs;
    /**
     * @return The batch size used during training.
     * 
     */
    private @Nullable Integer trainingBatchSize;
    /**
     * @return The fine-tuning method for training a custom model.
     * 
     */
    private String trainingConfigType;

    private ModelFineTuneDetailsTrainingConfig() {}
    /**
     * @return Stop training if the loss metric does not improve beyond &#39;early_stopping_threshold&#39; for this many times of evaluation.
     * 
     */
    public Optional<Integer> earlyStoppingPatience() {
        return Optional.ofNullable(this.earlyStoppingPatience);
    }
    /**
     * @return How much the loss must improve to prevent early stopping.
     * 
     */
    public Optional<Double> earlyStoppingThreshold() {
        return Optional.ofNullable(this.earlyStoppingThreshold);
    }
    /**
     * @return The initial learning rate to be used during training
     * 
     */
    public Optional<Double> learningRate() {
        return Optional.ofNullable(this.learningRate);
    }
    /**
     * @return Determines how frequently to log model metrics.
     * 
     * Every step is logged for the first 20 steps and then follows this parameter for log frequency. Set to 0 to disable logging the model metrics.
     * 
     */
    public Optional<Integer> logModelMetricsIntervalInSteps() {
        return Optional.ofNullable(this.logModelMetricsIntervalInSteps);
    }
    /**
     * @return This parameter represents the scaling factor for the weight matrices in LoRA.
     * 
     */
    public Optional<Integer> loraAlpha() {
        return Optional.ofNullable(this.loraAlpha);
    }
    /**
     * @return This parameter indicates the dropout probability for LoRA layers.
     * 
     */
    public Optional<Double> loraDropout() {
        return Optional.ofNullable(this.loraDropout);
    }
    /**
     * @return This parameter represents the LoRA rank of the update matrices.
     * 
     */
    public Optional<Integer> loraR() {
        return Optional.ofNullable(this.loraR);
    }
    /**
     * @return The number of last layers to be fine-tuned.
     * 
     */
    public Optional<Integer> numOfLastLayers() {
        return Optional.ofNullable(this.numOfLastLayers);
    }
    /**
     * @return The maximum number of training epochs to run for.
     * 
     */
    public Optional<Integer> totalTrainingEpochs() {
        return Optional.ofNullable(this.totalTrainingEpochs);
    }
    /**
     * @return The batch size used during training.
     * 
     */
    public Optional<Integer> trainingBatchSize() {
        return Optional.ofNullable(this.trainingBatchSize);
    }
    /**
     * @return The fine-tuning method for training a custom model.
     * 
     */
    public String trainingConfigType() {
        return this.trainingConfigType;
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(ModelFineTuneDetailsTrainingConfig defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private @Nullable Integer earlyStoppingPatience;
        private @Nullable Double earlyStoppingThreshold;
        private @Nullable Double learningRate;
        private @Nullable Integer logModelMetricsIntervalInSteps;
        private @Nullable Integer loraAlpha;
        private @Nullable Double loraDropout;
        private @Nullable Integer loraR;
        private @Nullable Integer numOfLastLayers;
        private @Nullable Integer totalTrainingEpochs;
        private @Nullable Integer trainingBatchSize;
        private String trainingConfigType;
        public Builder() {}
        public Builder(ModelFineTuneDetailsTrainingConfig defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.earlyStoppingPatience = defaults.earlyStoppingPatience;
    	      this.earlyStoppingThreshold = defaults.earlyStoppingThreshold;
    	      this.learningRate = defaults.learningRate;
    	      this.logModelMetricsIntervalInSteps = defaults.logModelMetricsIntervalInSteps;
    	      this.loraAlpha = defaults.loraAlpha;
    	      this.loraDropout = defaults.loraDropout;
    	      this.loraR = defaults.loraR;
    	      this.numOfLastLayers = defaults.numOfLastLayers;
    	      this.totalTrainingEpochs = defaults.totalTrainingEpochs;
    	      this.trainingBatchSize = defaults.trainingBatchSize;
    	      this.trainingConfigType = defaults.trainingConfigType;
        }

        @CustomType.Setter
        public Builder earlyStoppingPatience(@Nullable Integer earlyStoppingPatience) {

            this.earlyStoppingPatience = earlyStoppingPatience;
            return this;
        }
        @CustomType.Setter
        public Builder earlyStoppingThreshold(@Nullable Double earlyStoppingThreshold) {

            this.earlyStoppingThreshold = earlyStoppingThreshold;
            return this;
        }
        @CustomType.Setter
        public Builder learningRate(@Nullable Double learningRate) {

            this.learningRate = learningRate;
            return this;
        }
        @CustomType.Setter
        public Builder logModelMetricsIntervalInSteps(@Nullable Integer logModelMetricsIntervalInSteps) {

            this.logModelMetricsIntervalInSteps = logModelMetricsIntervalInSteps;
            return this;
        }
        @CustomType.Setter
        public Builder loraAlpha(@Nullable Integer loraAlpha) {

            this.loraAlpha = loraAlpha;
            return this;
        }
        @CustomType.Setter
        public Builder loraDropout(@Nullable Double loraDropout) {

            this.loraDropout = loraDropout;
            return this;
        }
        @CustomType.Setter
        public Builder loraR(@Nullable Integer loraR) {

            this.loraR = loraR;
            return this;
        }
        @CustomType.Setter
        public Builder numOfLastLayers(@Nullable Integer numOfLastLayers) {

            this.numOfLastLayers = numOfLastLayers;
            return this;
        }
        @CustomType.Setter
        public Builder totalTrainingEpochs(@Nullable Integer totalTrainingEpochs) {

            this.totalTrainingEpochs = totalTrainingEpochs;
            return this;
        }
        @CustomType.Setter
        public Builder trainingBatchSize(@Nullable Integer trainingBatchSize) {

            this.trainingBatchSize = trainingBatchSize;
            return this;
        }
        @CustomType.Setter
        public Builder trainingConfigType(String trainingConfigType) {
            if (trainingConfigType == null) {
              throw new MissingRequiredPropertyException("ModelFineTuneDetailsTrainingConfig", "trainingConfigType");
            }
            this.trainingConfigType = trainingConfigType;
            return this;
        }
        public ModelFineTuneDetailsTrainingConfig build() {
            final var _resultValue = new ModelFineTuneDetailsTrainingConfig();
            _resultValue.earlyStoppingPatience = earlyStoppingPatience;
            _resultValue.earlyStoppingThreshold = earlyStoppingThreshold;
            _resultValue.learningRate = learningRate;
            _resultValue.logModelMetricsIntervalInSteps = logModelMetricsIntervalInSteps;
            _resultValue.loraAlpha = loraAlpha;
            _resultValue.loraDropout = loraDropout;
            _resultValue.loraR = loraR;
            _resultValue.numOfLastLayers = numOfLastLayers;
            _resultValue.totalTrainingEpochs = totalTrainingEpochs;
            _resultValue.trainingBatchSize = trainingBatchSize;
            _resultValue.trainingConfigType = trainingConfigType;
            return _resultValue;
        }
    }
}
